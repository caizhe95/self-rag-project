# test_monitor.py
import functools
import time
from typing import Callable, Any


def monitor_retrieval(func: Callable) -> Callable:
    """ç›‘æ§æ£€ç´¢è´¨é‡ - è£…é¥°å™¨æ¨¡å¼"""

    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        query = kwargs.get("query", "") or (args[1] if len(args) > 1 else "")

        start = time.perf_counter()
        docs = await func(*args, **kwargs)
        duration = time.perf_counter() - start

        # è®¡ç®—ç›¸å…³æ€§
        if docs and query:
            scores = []
            for doc in docs:
                score = (doc.metadata.get("rerank_score") or
                        doc.metadata.get("similarity", 0.0) or
                        doc.metadata.get("hybrid_score", 0.0))
                if isinstance(score, (int, float)):
                    scores.append(float(score))
            relevance = max(scores) if scores else 0.0
        else:
            relevance = 0.0

        TestMetrics.record("retrieval", {
            "query": query,
            "duration": duration,
            "relevance": relevance,
            "docs_count": len(docs)
        })

        return docs

    return wrapper


def monitor_evaluation(func: Callable) -> Callable:
    """ç›‘æ§è¯„ä¼°è´¨é‡ - è£…é¥°å™¨æ¨¡å¼"""

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        review = func(*args, **kwargs)

        TestMetrics.record("evaluation", {
            "confidence": review.confidence,
            "hallucination_risk": review.hallucination_risk,
            "needs_human_review": review.needs_human_review
        })

        return review

    return wrapper


class TestMetrics:
    """å…¨å±€æµ‹è¯•æŒ‡æ ‡å­˜å‚¨"""
    data = {"retrieval": [], "evaluation": []}

    @classmethod
    def record(cls, category: str, metrics: dict):
        cls.data[category].append(metrics)

    @classmethod
    def get_report(cls) -> dict:
        if not cls.data["retrieval"] or not cls.data["evaluation"]:
            return {"total_queries": 0}

        return {
            "retrieval_avg_relevance": sum(r["relevance"] for r in cls.data["retrieval"]) / len(cls.data["retrieval"]),
            "retrieval_avg_duration": sum(r["duration"] for r in cls.data["retrieval"]) / len(cls.data["retrieval"]),
            "evaluation_avg_confidence": sum(e["confidence"] for e in cls.data["evaluation"]) / len(cls.data["evaluation"]),
            "evaluation_avg_hallucination": sum(e["hallucination_risk"] for e in cls.data["evaluation"]) / len(cls.data["evaluation"]),
            "total_queries": len(cls.data["evaluation"])
        }

    @classmethod
    def get_detailed_report(cls) -> str:
        """ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Š"""
        report = cls.get_report()
        if report["total_queries"] == 0:
            return "æš‚æ— ç›‘æ§æ•°æ®"

        lines = [
            "\n" + "=" * 60,
            "ğŸ“Š Self-RAG æ€§èƒ½ç›‘æ§æŠ¥å‘Š",
            "=" * 60,
            f"ğŸ“ æ€»æŸ¥è¯¢æ¬¡æ•°: {report['total_queries']}",
            "",
            "ã€æ£€ç´¢æ€§èƒ½ã€‘",
            f"  â±ï¸  å¹³å‡è€—æ—¶: {report['retrieval_avg_duration']*1000:.1f}ms",
            f"  ğŸ¯ å¹³å‡ç›¸å…³æ€§: {report['retrieval_avg_relevance']:.2f}",
            "",
            "ã€è¯„ä¼°è´¨é‡ã€‘",
            f"  âœ… å¹³å‡ç½®ä¿¡åº¦: {report['evaluation_avg_confidence']:.2f}",
            f"  âš ï¸  å¹³å‡å¹»è§‰é£é™©: {report['evaluation_avg_hallucination']:.2f}",
            "=" * 60
        ]
        return "\n".join(lines)

    @classmethod
    def reset(cls):
        cls.data = {"retrieval": [], "evaluation": []}