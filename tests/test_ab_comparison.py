# test_ab_comparison.py - å®Œæ•´ç‰ˆABå¯¹æ¯”æµ‹è¯•

import asyncio
import requests
import json
import time
from typing import List, Dict, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from statistics import mean

API_URL = "http://localhost:8000"


@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""
    name: str
    hybrid_weights: Dict[str, float]
    reranker_enabled: bool
    description: str


class ABComparisonTester:
    """ABå¯¹æ¯”æµ‹è¯•å™¨"""

    def __init__(self):
        self.api_url = API_URL
        self.configs = [
            TestConfig(
                name="çº¯å‘é‡æ£€ç´¢",
                hybrid_weights={"bm25": 0.0, "vector": 1.0},
                reranker_enabled=False,
                description="Chromaå‘é‡ç›¸ä¼¼åº¦"
            ),
            TestConfig(
                name="æ··åˆæ£€ç´¢(BM25+å‘é‡)",
                hybrid_weights={"bm25": 0.4, "vector": 0.6},
                reranker_enabled=False,
                description="BM40%+å‘é‡60%ï¼Œæ— é‡æ’åº"
            ),
            TestConfig(
                name="æ··åˆ+é‡æ’åº",
                hybrid_weights={"bm25": 0.4, "vector": 0.6},
                reranker_enabled=True,
                description="BM25+å‘é‡+Cross-Encoder"
            ),
        ]

        self.test_queries = [
            # ai_ethics.md
            "ç®—æ³•åè§æ€ä¹ˆè§£å†³",
            "AIéšç§ä¿æŠ¤æŠ€æœ¯æœ‰å“ªäº›",

            # ai_history.md
            "Transformeræ˜¯å“ªä¸€å¹´æå‡ºçš„",
            "ChatGPTå‘å±•å†ç¨‹",

            # deep_learning_arch.md
            "CNNå’ŒRNNæœ‰ä»€ä¹ˆåŒºåˆ«",
            "æ³¨æ„åŠ›æœºåˆ¶åŸç†",

            # llm_training.md
            "RLHFè®­ç»ƒæµç¨‹",
            "LoRAå¾®è°ƒä¼˜åŠ¿",

            # ml_basics.md
            "è¿‡æ‹Ÿåˆè§£å†³æ–¹æ³•",
            "ç›‘ç£å­¦ä¹ åº”ç”¨åœºæ™¯",

            # è·¨æ–‡æ¡£
            "æ·±åº¦å­¦ä¹ å‘å±•å†å²",
            "å¤§æ¨¡å‹ä¼¦ç†é—®é¢˜",
        ]

        self.results: List[Dict] = []

    async def run(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("=" * 80)
        print("ğŸ§ª ABå¯¹æ¯”æµ‹è¯•ï¼šé‡åŒ–æ··åˆæ£€ç´¢ä¸é‡æ’åºçš„çœŸå®æå‡")
        print("=" * 80)
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(
            f"æŸ¥è¯¢æ•°: {len(self.test_queries)} Ã— {len(self.configs)}é…ç½® = {len(self.test_queries) * len(self.configs)}æ¬¡")
        print("-" * 80)

        # æ£€æŸ¥æœåŠ¡
        if not await self._check_service():
            print("âŒ æœåŠ¡ä¸å¯ç”¨")
            return

        # æµ‹è¯•æ¯ä¸ªé…ç½®
        for config in self.configs:
            print(f"\n{'=' * 80}")
            print(f"ğŸ“‹ é…ç½®: {config.name}")
            print(f"   {config.description}")
            print("-" * 80)

            config_results = await self._test_config(config)
            self.results.append({
                "config": asdict(config),
                "results": config_results
            })

        # ç”ŸæˆæŠ¥å‘Š
        return self._generate_report()

    async def _check_service(self) -> bool:
        """æ£€æŸ¥æœåŠ¡çŠ¶æ€"""
        try:
            r = requests.get(f"{self.api_url}/health", timeout=5)
            if r.status_code == 200:
                data = r.json()
                print(f"âœ… æœåŠ¡æ­£å¸¸")
                print(f"   æ¨¡å‹: {data.get('model', 'unknown')}")
                print(f"   æ–‡æ¡£æ•°: {data.get('document_count', 0)}")
                return True
        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return False

    async def _test_config(self, config: TestConfig) -> List[Dict]:
        """æµ‹è¯•å•ä¸ªé…ç½®"""
        results = []

        # åº”ç”¨é…ç½®
        if not self._apply_config(config):
            print(f"   âš ï¸ é…ç½®åº”ç”¨å¤±è´¥ï¼Œè·³è¿‡")
            return results

        # ç­‰å¾…é…ç½®ç”Ÿæ•ˆ
        await asyncio.sleep(0.5)

        for idx, query in enumerate(self.test_queries, 1):
            print(f"\n   [{idx}/{len(self.test_queries)}] '{query}'")

            try:
                # ä½¿ç”¨debugæ¥å£è·å–è¯¦ç»†ä¿¡æ¯
                r = requests.post(
                    f"{self.api_url}/api/retrieval/debug",
                    params={"query": query},
                    timeout=30
                )

                if r.status_code != 200:
                    print(f"      âŒ APIé”™è¯¯: {r.status_code}")
                    continue

                data = r.json()
                if not data.get("success"):
                    print(f"      âŒ è¯·æ±‚å¤±è´¥")
                    continue

                # è§£ææŒ‡æ ‡
                metrics = data.get("metrics", {})
                docs = data.get("retrieved_docs", [])

                result = {
                    "query": query,
                    "vector_count": data.get("vector_count", 0),
                    "bm25_count": data.get("bm25_count", 0),
                    "final_count": data.get("final_count", 0),
                    "vector_time_ms": metrics.get("vector_time_ms", 0),
                    "bm25_time_ms": metrics.get("bm25_time_ms", 0),
                    "rerank_time_ms": metrics.get("rerank_time_ms", 0),
                    "total_time_ms": metrics.get("total_time_ms", 0),
                    "docs": docs,
                    "top_scores": {
                        "vector": max([d.get("vector_score", 0) for d in docs if d.get("vector_score")], default=0),
                        "bm25": max([d.get("bm25_score", 0) for d in docs if d.get("bm25_score")], default=0),
                        "rerank": max([d.get("rerank_score", 0) for d in docs if d.get("rerank_score")], default=0),
                        "final": max([d.get("final_score", 0) for d in docs], default=0) if docs else 0,
                    }
                }
                results.append(result)

                # å®æ—¶æ˜¾ç¤º
                print(f"      â±ï¸  {result['total_time_ms']:.0f}ms "
                      f"| ğŸ“„ V:{result['vector_count']} B:{result['bm25_count']} F:{result['final_count']}"
                      f"| ğŸ¯ æœ€é«˜åˆ†:{result['top_scores']['final']:.3f}")

            except Exception as e:
                print(f"      âŒ é”™è¯¯: {e}")

        return results

    def _apply_config(self, config: TestConfig) -> bool:
        """åº”ç”¨é…ç½®åˆ°æœåŠ¡å™¨"""
        try:
            r = requests.post(
                f"{self.api_url}/api/config/retrieval",
                json={
                    "hybrid_weights": config.hybrid_weights,
                    "reranker_enabled": config.reranker_enabled
                },
                timeout=10
            )
            if r.status_code == 200:
                print(f"   âœ… é…ç½®å·²åº”ç”¨: {config.hybrid_weights}, rerank={config.reranker_enabled}")
                return True
            else:
                print(f"   âŒ é…ç½®å¤±è´¥: {r.status_code}")
                return False
        except Exception as e:
            print(f"   âŒ é…ç½®å¼‚å¸¸: {e}")
            return False

    def _generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ABå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)

        if len(self.results) != 3:
            print("âŒ æ•°æ®ä¸å®Œæ•´")
            return {}

        # æå–ä¸‰ç§é…ç½®
        pure_vector = self.results[0]["results"]
        hybrid = self.results[1]["results"]
        hybrid_rerank = self.results[2]["results"]

        # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        def calc_metrics(results: List[Dict]) -> Dict:
            if not results:
                return {}
            return {
                "avg_time_ms": mean([r["total_time_ms"] for r in results]),
                "avg_vector_count": mean([r["vector_count"] for r in results]),
                "avg_bm25_count": mean([r["bm25_count"] for r in results]),
                "avg_final_count": mean([r["final_count"] for r in results]),
                "avg_top_score": mean([r["top_scores"]["final"] for r in results]),
            }

        m1 = calc_metrics(pure_vector)
        m2 = calc_metrics(hybrid)
        m3 = calc_metrics(hybrid_rerank)

        # æ‰“å°å¯¹æ¯”è¡¨
        print("\nã€æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”ã€‘")
        print(f"{'æŒ‡æ ‡':<25} {'çº¯å‘é‡':<15} {'æ··åˆæ£€ç´¢':<15} {'æ··åˆ+é‡æ’':<15} {'æ··åˆæå‡':<12} {'é‡æ’æå‡':<12}")
        print("-" * 100)

        def fmt(val, unit=""):
            return f"{val:.1f}{unit}" if isinstance(val, float) else str(val)

        def calc_imp(base, new):
            return f"{(new / base - 1) * 100:+.1f}%" if base > 0 else "N/A"

        rows = [
            ("å¹³å‡å“åº”æ—¶é—´", "ms", m1.get("avg_time_ms", 0), m2.get("avg_time_ms", 0), m3.get("avg_time_ms", 0)),
            ("å¬å›æ–‡æ¡£æ•°", "ç¯‡", m1.get("avg_final_count", 0), m2.get("avg_final_count", 0),
             m3.get("avg_final_count", 0)),
            ("Top1ç›¸å…³æ€§åˆ†æ•°", "", m1.get("avg_top_score", 0), m2.get("avg_top_score", 0), m3.get("avg_top_score", 0)),
            ("å‘é‡æ£€ç´¢è€—æ—¶", "ms", m1.get("avg_time_ms", 0), m2.get("avg_vector_count", 0) * 0,
             m3.get("avg_vector_count", 0) * 0),  # å ä½
        ]

        for name, unit, v1, v2, v3 in rows[:3]:
            imp1 = calc_imp(v1, v2)
            imp2 = calc_imp(v2, v3)
            print(f"{name:<25} {fmt(v1, unit):<15} {fmt(v2, unit):<15} {fmt(v3, unit):<15} {imp1:<12} {imp2:<12}")

        # è¯¦ç»†åˆ†æ
        print("\nã€è¯¦ç»†åˆ†æã€‘")

        # å¬å›æ•°é‡å¯¹æ¯”
        recall_pure = m1["avg_final_count"]
        recall_hybrid = m2["avg_final_count"]
        recall_boost = (recall_hybrid / recall_pure - 1) * 100 if recall_pure > 0 else 0

        print(f"\n1ï¸âƒ£ å¬å›æ•°é‡")
        print(f"   çº¯å‘é‡: {recall_pure:.1f}ç¯‡")
        print(f"   æ··åˆæ£€ç´¢: {recall_hybrid:.1f}ç¯‡ (æå‡ {recall_boost:+.1f}%)")
        print(f"   âœ… BM25è¡¥å……äº†å‘é‡æ£€ç´¢æœªè¦†ç›–çš„æ–‡æ¡£")

        # ç›¸å…³æ€§å¯¹æ¯”
        score_pure = m1["avg_top_score"]
        score_hybrid = m2["avg_top_score"]
        score_rerank = m3["avg_top_score"]

        hybrid_boost = (score_hybrid / score_pure - 1) * 100 if score_pure > 0 else 0
        rerank_boost = (score_rerank / score_hybrid - 1) * 100 if score_hybrid > 0 else 0

        print(f"\n2ï¸âƒ£ ç›¸å…³æ€§è´¨é‡ (Top1åˆ†æ•°)")
        print(f"   çº¯å‘é‡: {score_pure:.3f}")
        print(f"   æ··åˆæ£€ç´¢: {score_hybrid:.3f} (æå‡ {hybrid_boost:+.1f}%)")
        print(f"   æ··åˆ+é‡æ’: {score_rerank:.3f} (å†æå‡ {rerank_boost:+.1f}%)")
        print(f"   âœ… æ··åˆæ£€ç´¢é€šè¿‡åŠ æƒèåˆæå‡ç›¸å…³æ€§")
        print(f"   âœ… Cross-Encoderé‡æ’åºè¿›ä¸€æ­¥ä¼˜åŒ–TopKè´¨é‡")

        # æ€§èƒ½å¯¹æ¯”
        time_pure = m1["avg_time_ms"]
        time_hybrid = m2["avg_time_ms"]
        time_rerank = m3["avg_time_ms"]

        time_overhead = (time_rerank / time_pure - 1) * 100 if time_pure > 0 else 0

        print(f"\n3ï¸âƒ£ å“åº”æ—¶é—´")
        print(f"   çº¯å‘é‡: {time_pure:.0f}ms")
        print(f"   æ··åˆæ£€ç´¢: {time_hybrid:.0f}ms")
        print(f"   æ··åˆ+é‡æ’: {time_rerank:.0f}ms ( overhead {time_overhead:+.1f}%)")
        print(f"   â±ï¸  é‡æ’åºå¢åŠ çº¦ {m3['avg_time_ms'] - m2['avg_time_ms']:.0f}ms")

        # ç»“è®º
        print("\nã€æ ¸å¿ƒç»“è®ºã€‘")
        print(f"  âœ… æ··åˆæ£€ç´¢å¬å›æ•°é‡æå‡ {recall_boost:.1f}%")
        print(f"  âœ… æ··åˆæ£€ç´¢ç›¸å…³æ€§æå‡ {hybrid_boost:.1f}%")
        print(f"  âœ… é‡æ’åºé¢å¤–æå‡ç›¸å…³æ€§ {rerank_boost:.1f}%")
        if time_overhead < 50:
            print(f"  âœ… æ—¶é—´å¼€é”€ä»… {time_overhead:.1f}%ï¼Œæ€§ä»·æ¯”é«˜")
        else:
            print(f"  âš ï¸  æ—¶é—´å¼€é”€ {time_overhead:.1f}%ï¼Œéœ€æƒè¡¡")

        print("\nã€é¢è¯•æ•°æ®ã€‘")
        print(f'  "æ··åˆæ£€ç´¢å¬å›æ•°é‡æå‡{recall_boost:.0f}%ï¼Œé‡æ’åºåTop3ç›¸å…³æ€§æå‡{rerank_boost:.0f}%ï¼Œ')
        print(f'   å“åº”æ—¶é—´å¢åŠ {time_overhead:.0f}%ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…ã€‚"')

        print("=" * 80)

        # æ¢å¤é…ç½®
        try:
            requests.post(f"{self.api_url}/api/config/retrieval/reset", timeout=5)
            print("\nğŸ”„ é…ç½®å·²æ¢å¤")
        except:
            pass

        return {
            "timestamp": datetime.now().isoformat(),
            "configs": self.results,
            "summary": {
                "recall_boost_percent": round(recall_boost, 1),
                "hybrid_score_boost_percent": round(hybrid_boost, 1),
                "rerank_score_boost_percent": round(rerank_boost, 1),
                "time_overhead_percent": round(time_overhead, 1),
            },
            "metrics": {
                "pure_vector": m1,
                "hybrid": m2,
                "hybrid_rerank": m3
            }
        }


async def main():
    tester = ABComparisonTester()
    report = await tester.run()

    # ä¿å­˜
    filename = f"ab_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {filename}")


if __name__ == "__main__":
    asyncio.run(main())