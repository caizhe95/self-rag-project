# 深度学习架构演进：从CNN到Transformer

## CNN（卷积神经网络）2012-2015
- **核心思想**：局部感知、权值共享
- **典型模型**：AlexNet、VGG、ResNet
- **优势**：平移不变性，适合图像处理
- **局限**：无法建模长距离依赖

## RNN/LSTM（循环神经网络）2014-2017
- **核心思想**：序列建模，记忆历史信息
- **典型模型**：LSTM、GRU
- **优势**：处理变长序列，适合NLP
- **局限**：梯度消失/爆炸，难以并行化

## Transformer（2017-至今）
- **核心思想**：自注意力机制，全局建模
- **关键技术**：
  - 多头注意力（Multi-Head Attention）
  - 位置编码（Positional Encoding）
  - 前馈网络（Feed-Forward Networks）
- **优势**：并行计算，捕捉长距离依赖
- **代表模型**：BERT、GPT系列

## 架构对比

| 特性 | CNN | RNN | Transformer |
|------|-----|-----|-------------|
| **计算方式** | 卷积 | 循环 | 注意力 |
| **并行性** | 高 | 低 | 极高 |
| **长程依赖** | 差 | 中等 | 优秀 |
| **数据类型** | 图像为主 | 序列为主 | 通用 |
| **计算量** | 中等 | 低 | 高 |

## 大模型架构趋势

- **MoE（混合专家）**：稀疏激活，提升参数效率
- **RetNet**：结合RNN和Transformer优点
- **Mamba**：状态空间模型，线性复杂度
- **轻量化**：MobileNet、EfficientNet适配边缘设备

## 未来方向

- **效率与性能平衡**：更少参数达到更好效果
- **多模态统一**：单一架构处理文本、图像、语音
- **动态架构**：根据输入自适应调整计算路径