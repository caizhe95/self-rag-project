# core/document_processor.pyï¼ˆç”Ÿäº§çº§å®ç°ï¼‰
import asyncio
import time
from pathlib import Path
from typing import List, Dict, Optional, Union, Any

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

from core.ocr_processor import OCRProcessor
from storage.vector_store import VectorStoreManager


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ï¼šåŠ è½½ã€åˆ†å—ã€OCRã€å‘é‡åŒ–ï¼ˆç”Ÿäº§çº§å®ç°ï¼‰"""

    def __init__(self, config):
        self.config = config

        # æ™ºèƒ½åˆ†å—å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )

        # OCRå¤„ç†å™¨
        self.ocr_processor = OCRProcessor(config.ocr_language, config.ocr_enabled)

        # å‘é‡å­˜å‚¨ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.vector_manager: Optional[VectorStoreManager] = None
        self.vector_store: Optional[Any] = None

    def _load_text_documents(self, texts: List[str], metadatas: List[Dict]) -> List[Document]:
        """åŠ è½½çº¯æ–‡æœ¬æ–‡æ¡£"""
        documents = []
        for i, text in enumerate(texts):
            metadata = metadatas[i] if i < len(metadatas) else {"source": f"doc_{i}"}
            documents.append(Document(page_content=text, metadata=metadata))
        return documents

    async def _load_file_documents(self, files: List[Path]) -> List[Document]:
        """åŠ è½½æ–‡ä»¶æ–‡æ¡£ï¼ˆå¸¦OCRï¼‰"""
        if not files or not self.ocr_processor.is_available():
            return []

        print(f"ğŸ“· OCRå¤„ç†ï¼š{len(files)}ä¸ªæ–‡ä»¶")
        docs = []
        for file_path in files:
            text = await self.ocr_processor.extract_text(file_path)
            if text:
                doc = self.ocr_processor.create_document(file_path, text)
                docs.append(doc)
                print(f"   âœ“ {file_path.name}: æå–äº†{len(text)}å­—ç¬¦")
        return docs

    def _split_documents(self, documents: List[Document]) -> List[Document]:
        """æ™ºèƒ½åˆ†å—ï¼ˆå¸¦å…ƒæ•°æ®å¢å¼ºï¼‰"""
        all_chunks = []
        for idx, doc in enumerate(documents):
            # åˆ¤æ–­æ–‡æ¡£ç±»å‹
            source = doc.metadata.get("source", "")
            is_markdown = source.endswith((".md", ".markdown"))
            is_code = source.endswith((".py", ".js", ".java"))

            if is_markdown:
                # Markdownåˆ†å—
                from langchain_text_splitters import MarkdownHeaderTextSplitter
                md_splitter = MarkdownHeaderTextSplitter(
                    headers_to_split_on=[("#", "Header1"), ("##", "Header2")]
                )
                chunks = md_splitter.split_text(doc.page_content)
            elif is_code:
                # ä»£ç åˆ†å—
                from langchain_text_splitters import RecursiveCharacterTextSplitter, Language
                ext = source.split(".")[-1]
                language_map = {"py": Language.PYTHON, "js": Language.JS, "java": Language.JAVA}
                code_splitter = RecursiveCharacterTextSplitter.from_language(
                    language=language_map.get(ext, Language.PYTHON),
                    chunk_size=800,
                    chunk_overlap=0
                )
                chunks = code_splitter.split_documents([doc])
            else:
                # é»˜è®¤åˆ†å—
                chunks = self.text_splitter.split_documents([doc])

            # å¢å¼ºå…ƒæ•°æ®
            for i, chunk in enumerate(chunks):
                chunk.metadata.update({
                    "parent_id": f"doc_{idx}",
                    "splitter": "markdown" if is_markdown else "code" if is_code else "fallback",
                    "chunk_idx": i,
                })

            all_chunks.extend(chunks)

        print(f"âœ‚ï¸  åˆ†å—å®Œæˆï¼š{len(all_chunks)}ä¸ªæ–‡æœ¬å—")
        return all_chunks

    async def process(self, texts: List[str], files: Optional[List[Union[Path, str]]] = None,
                      metadatas: Optional[List[Dict]] = None) -> Any:
        """å®Œæ•´å¤„ç†æµç¨‹ï¼šåŠ è½½ â†’ åˆ†å— â†’ å‘é‡åŒ–"""
        print("ğŸ“„ åŠ è½½æ–‡æ¡£...")

        # 1. å¤„ç†çº¯æ–‡æœ¬
        documents = self._load_text_documents(texts, metadatas or [])

        # 2. å¤„ç†æ–‡ä»¶ï¼ˆOCRï¼‰
        if files:
            file_docs = await self._load_file_documents([Path(f) for f in files])
            documents.extend(file_docs)

        print(f"   æ€»è®¡ {len(documents)} ä¸ªæ–‡æ¡£")

        # 3. æ™ºèƒ½åˆ†å—
        chunks = self._split_documents(documents)
        print(f"   ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")

        # 4. åˆ›å»ºå‘é‡å­˜å‚¨
        print("ğŸ”¢ åˆ›å»ºå‘é‡å­˜å‚¨...")

        # åˆå§‹åŒ– vector_manager
        if self.vector_manager is None:
            self.vector_manager = VectorStoreManager(self.config)

        vector_store = self.vector_manager.create_from_documents(chunks)
        print("   å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ")

        self.vector_store = vector_store

        return vector_store

    # ==================== æ–°å¢ï¼šç”Ÿäº§çº§OCRæ–‡æ¡£æ·»åŠ æ–¹æ³• ====================
    async def add_ocr_documents(self, files: List[Union[Path, str]]) -> List[Document]:
        """ç”Ÿäº§çº§ï¼šåŠ¨æ€æ·»åŠ OCRæ–‡æ¡£åˆ°å‘é‡åº“"""
        if not self.ocr_processor.is_available():
            print("âš ï¸  OCRåŠŸèƒ½ä¸å¯ç”¨ï¼Œæ— æ³•æ·»åŠ OCRæ–‡æ¡£")
            return []

        if not self.vector_manager or not self.vector_store:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ process()")

        print(f"ğŸ“· åŠ¨æ€æ·»åŠ OCRæ–‡æ¡£ï¼š{len(files)}ä¸ªæ–‡ä»¶")

        ocr_docs = []
        for file_path in files:
            text = await self.ocr_processor.extract_text(Path(file_path))
            if text:
                doc = self.ocr_processor.create_document(Path(file_path), text)
                # ä¿®æ”¹metadataæ ‡è®°ä¸ºOCRæ–‡æ¡£
                doc.metadata.update({
                    "source": f"ocr_{Path(file_path).name}",
                    "doc_type": "ocr_document",
                    "added_at": time.time()
                })
                ocr_docs.append(doc)
                print(f"   âœ“ {Path(file_path).name}: æå–äº†{len(text)}å­—ç¬¦")

        if ocr_docs:
            # æ·»åŠ åˆ°å‘é‡åº“
            self.vector_manager.add_documents(ocr_docs)

            # æ›´æ–°æ£€ç´¢å™¨ï¼ˆå¿…é¡»é‡æ–°åˆå§‹åŒ–BM25ï¼‰
            if self.retriever:
                self.retriever.update_documents(ocr_docs)

            print(f"âœ… æˆåŠŸæ·»åŠ  {len(ocr_docs)} ä¸ªOCRæ–‡æ¡£")

        return ocr_docs
    # ==================== æ–°å¢ç»“æŸ ====================